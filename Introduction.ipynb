{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook, we'll be making notes about the initial chapters of Deep Learning for Computer Vision with Python, Vol 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning has been around for a long time, but now with better hardware and more data it has become ubiquitous.\n",
    "### Deep Learning vs other kinds of artificial intelligence\n",
    "Before the deep learning advent, many different features were taken from a image so the artificial network could process it. Some examples were countours, edges, color groups and other features. With deep learning, there's no need for that, we just put the raw pixels into the network and the features are automatically learned. Each layer represents a concept the goes up in abstraction, the first could be the edges of the image, the second the contours the third the objects and so on.\n",
    "In Adrian's opinion everything with more than two layers should be called deep learning, and with more than 10 very deep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Fundamentals\n",
    "Numpy represents the pixels in a 3D format, and they are width, height and depth. When we talk about depth, it's about the color channels that the image has. Another thing that is different than the normal is that width and height are really in this ordering, because that's how a matrix is represented.\n",
    "##### BGR Ordering\n",
    "Because of historical reasons opencv uses BGR ordering in colors, instead of the normal RGB.\n",
    "##### Neural Network algorithms image size\n",
    "Most of the algorithms use some fixed size for the images, some common choices are 32×32, 64×64, 224×224, 227×227, 256×256, and 299×299."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Classification\n",
    "In machine learning we have the dataset and the so called data points, that is each individual image.\n",
    "#### Steps in image classification\n",
    "##### Dataset gathering\n",
    "We need to have the images that will be used, and all of the groups should have the same quantity of images.\n",
    "##### Splitting the dataset\n",
    "The dataset should be splitten in 3 different parts, and it's very important that none of these overlap. They are:\n",
    "Training Set: The dataset used to train the neural network\n",
    "Validation Set: The dataset used to set the hyperparameters (as learning rate, decay, etc)\n",
    "Testing Set: The dataset that will be used to evaluate the NN\n",
    "Some common divisions are: From 66.7% to 90% are reserved to the training and validation sets, while the validation takes between 10 and 20% from this space,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
